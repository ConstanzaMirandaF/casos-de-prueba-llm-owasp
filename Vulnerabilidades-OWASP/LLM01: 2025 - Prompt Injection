# LLM01: 2025 - Prompt Injection 

## Descripción del ataque
Se produce cuando las instrucciones del usuario modifican el comportamiento o la salida del LLM de manera inesperada. Así, se intenta manipular las respuestas del modelo mediante entradas específicas para alterar su funcionamiento y, eventualmente, evadir las medidas o protocolos de seguridad.

Este tipo de vulnerabilidad se origina en la forma en que los modelos procesan las instrucciones del usuario, permitiendo que ciertos fragmentos de la entrada se propaguen de manera indebida hacia otras áreas del sistema. Esta transferencia incorrecta puede provocar que el modelo infrinja sus directrices internas, genere contenido perjudicial, habilite accesos no autorizados o influya de forma inapropiada en decisiones críticas.

## Ejemplos de esta vulnerabilidad
+ *Inyecciones directas de avisos*: Ocurren cuando la entrada (intencional o no intencional) de un usuario altera directamente el comportamiento del modelo de forma imprevista o inesperada.
+ *Inyecciones indirectas rápidas*: Ocurren cuando un LLM acepta información de fuentes externas, como sitios web o archivos. Pueden ser intencionales o no, así como también, se puede alterar el comportamiento del modelo de forma imprevista o inesperada.

## Consecuencias de esta vulneración
+ Divulgación de información sensible.
+ Revelar información confidencial sobre la infraestructura o las indicaciones del sistema.
+ Manipulación de contenido que genere resultados incorrectos o sesgados.
+ Proporcionar acceso no autorizado a funciones disponibles para el LLM.
+ Ejecutar comandos arbitrarios en sistemas conectados.
+ Manipular procesos críticos de toma de decisiones.
+ Impacto potencial en la seguridad, privacidad, funcionamiento o reputación del sistema.

## Estrategias de prevención y mitigación
+ *Restringir el comportamiento del modelo*: Consiste en aplicar mecanismos que limitan o condicionan la forma en que el modelo responde a las entradas del usuario. Esto puede incluir:
   + *Filtrado de entradas*: Se bloquean o modifican ciertas instrucciones consideradas inapropiadas, maliciosas o fuera de alcance, lo que puede percibirse como una forma de censura.
   + *Instrucciones de funcionamiento*: Se incorporan indicaciones explícitas sobre cómo debe comportarse el modelo, incluyendo normas éticas, límites temáticos o estilos de respuesta.
   + *Restricción de salidas*: El modelo puede negarse a responder ante ciertos tipos de solicitudes.
+ *Definir y validar los formatos de salida esperados*.
+ *Implementar el filtrado de entrada y salida*: Detección de contenido no permitido.
+ *Imponer el control de privilegios y el acceso con privilegios mínimos*: Restringir al mínimo los privilegios de acceso al modelo.
+ *Exigir aprobación humana para acciones de alto riesgo*.
+ *Segregar e identificar el contenido externo*: Limitar la influencia de las indicaciones del usuario.
+ *Realizar pruebas adversarias y simulaciones de ataques*.

## Relación con MITRE ATLAS

- Técnica MITRE ATLAS relacionada: 
- Descripción:
- Justificación del match: 

Vulnerabilidades que APLICAN para Latam GPT:
✅ LLM01:2025 - Prompt Injection: Aplicable - Los usuarios pueden manipular prompts para evadir filtros de seguridad mediante ataques adversariales
