# LLM09: 2025 - Misinformation

## Descripción del ataque
Los LLMs pueden generar información falsa o engañosa que parece factual, potencialmente propagando desinformación y erosionando la confianza.

## Ejemplos de esta vulnerabilidad
+ *Hallucination*: Generación de información falsa pero plausible.
+ *Factual Errors*: Errores en hechos y datos.
+ *Context Confusion*: Confusión de contextos y fuentes.
+ *Source Fabrication*: Creación de fuentes inexistentes.
+ *Temporal Inconsistency*: Inconsistencias temporales en información.

## Consecuencias de esta vulneración
Impacto potencial en la seguridad, privacidad, funcionamiento o reputación del sistema.

## Estrategias de prevención y mitigación.
+ Implementar verificadores de hechos robustos.
+ Usar fuentes de datos confiables y verificables.
+ Aplicar técnicas de grounding en información real.
+ Implementar sistemas de citación y referenciación.
+ Monitorear y detectar patrones de alucinación.

## Relación con MITRE ATLAS

- Técnica MITRE ATLAS relacionada: 
- Descripción:
- Justificación del match: 

## APLICA para Latam GPT:
✅ Aplicable? - El modelo puede generar desinformación debido a ataques adversariales específicos.
